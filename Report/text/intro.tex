%The introduction section of the report should introduce the project in
%more detail than in the abstract. In particular it should present the
%motivation, the aims, outline of techniques used, and the scope of the project. 
%It should also contain references to similar work in the
%same field to put your work in the correct context.
%
%As a general rule, people reading the abstract and introduction alone
%should have a good idea of the material in the project, the techniques
%employed and the results obtained. A typical introduction should be
%about 1 page, (300-450 words).

\section{Introduction}

Objectively comparing and ranking some set of entities is an important challenge. Consumers are daily tested to choose the right product, companies look for the most fruitful strategies to pursue, online search engines order query results by relevance and competitors seek to find who is the best. If there are clear criteria for comparison, ranking is straightforward. For example, marathons are always the same distance and with similar surface properties and topology. Some additional details may need to be taken into account like different age groups of the runners, but in general it is sufficient to compare the competitors' run rimes (historic or recent) in order to establish their ability.\\ 
However, not all ranking problems present clear criteria. In team sports and multi-player video games the landscape is more complicated. How do individual player contributions accumulate to reach the eventual victory or defeat? Is a team that barely wins or one that wins by a large margin better in the long run? [......]
One way to approach such systems is to abstract away from specific details and employ statistical methods. This report will examine the ranking scheme used in British orienteering competitions through computer simulation of ideal data. The scheme will then be applied to real race data and improvements will be sought.




%Theory and background
% short overview of theoretical background to project
% principles of experiment, no derivations

%Literature survery
% a review of relevant topics and work within the field (not just a list)
% some depth
% must provide added value beyond results of report
% annotated well
% avoid webpages if possible

\section{Background or Theory}
Clear criteria for arranging a group of entities in order of quality are not always available. In such cases it is desirable to infer the ranks from results of comparison events between the entities. The goal is to find each participant's skill level and to remove as much noise from it as possible. This will be referred to as skill based ranking.\\
A good starting point of skill based ranking is the Elo rating system\cite{elo}. It was developed by Arpad Elo and found its primary use in comparing chess players. The basic principle of the system is that each player's skill can be represented by a Gaussian curve. The player's skill is thought to be the mean of the curve and variation to it is allowed by the sloping sides. When two players enter a match, the likelihood if outcomes can be found by looking at the difference of both players' skill curves. The rank change of both players after the game are proportional to how unlikely that outcome was. The system has been applied to other sports as well, for example football\cite{football}.\\
A weakness of the Elo system is that it assigns an equal uncertainty to all players' ranks, this is addressed in the Glicko system\cite{glicko}. In this system a person who plays frequently is subject to smaller rank changes than someone who plays less frequently. As a player accumulates more and consistent games, the standard deviation around their mean rank shrinks.\\
A further extension to multiplayer games is Microsoft's TrueSkill which was developed for the Xbox gaming system\cite{trueskill}. The principle is very similar to Glicko, expected outcomes leave ranks almost unchanged while upsets cause big changes. Teams of players are assigned aggregate mean skill and uncertainty, the outcome of the match usually causes different individual rank changes for each player. Where more than two teams or players are competing all against each other, the participating entities are compared pairwise. The system also includes iterative steps that run the same scores through the system multiple times and different weights for game types based on how big of a role luck and skill plays. The exact details of the algorithm are rather involved and excellent explanations and a sample implementation can be found in other resources(\cite{trueskillMicrosoft}, \cite{trueskillSimple}). This ranking system has an elaborate theoretical basis and has shown good predictive power\cite{trueskill}. An interesting expansion of TrueSkill is AllegSkill\cite{allegskill} where additional provision exists for team commanders' skills.\\
A shared feature of the discussed ranking systems is that they are geared towards evaluation one versus one competitions. While all versus all ranking applications are possible, in practice they are executed as a series of one versus one comparisons. The ranking scheme used by British Orienteering Federation, the focus of this report, is an all versus all system in essence. The central equation of interest is the score assignment formula\cite{bof}:
\begin{equation}
RP = MP + \frac{SP * (MT - RT)}{ST}
\end{equation}
Where R stands for run, P for points, T for time, M for mean and S for standard deviation. The formula says that the number of points awarded to a runner in a race is the mean number of points of the runners participating in that race plus the standard deviation of the runners' points multiplied by the number of standard deviation that the runner being awarded is away from the mean time of all runners' times. The best guess of a runner's skill is the mean of their scores obtained races (published scores usually the mean of a subset of all scores).\\
It is tempting to include in the discussion the central limit theorem (CLT) which states that the distribution of means sampled from almost any distribution will tend to a Gaussian\cite{probability}. However, the theorem requires independent samples drawn from an identical distribution. In this case the distribution of scores changes after each recorded race, which also implies correlation between subsequent scores. Thus, while the CLT might apply in some states, in general it cannot be strictly applied to this system.\\
Another concept that must be mentioned is self consistency. One application of this principle is the Hartree-Fock algorithm. In the method a trial wavefunction is used to calculate the energy of a complicated quantum system. This energy is then used to adjust free variables in the wavefunction, which is then used again to gain a new estimate of the energy. The loop is continued until new approximations do not appreciably change, the system is then reached self consistency. This principle could be applied to orienteering ranking by putting race results back though the system multiple times, each time potentially improving the estimate of the players' ranks. It would be reassuring if the system eventually settled on a stable state instead of changing ranks continually.\\
Because systems similar to the one used by the British Orienteering Federation are not widely discussed in scientific literature, an approach based on simulations and experiments will be used to analyse the ranking scheme.


